import os
import warnings

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (import ì „ì— ì‹¤í–‰)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["PYTHONWARNINGS"] = "ignore"

# ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Union
from dotenv import load_dotenv
import asyncio
import json
import time
from datetime import datetime
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# LangChain imports (í˜¸í™˜ì„± í™•ì¸ëœ ìˆœì„œë¡œ import)
try:
    from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.vectorstores import Chroma
    from langchain_community.retrievers import BM25Retriever
    from langchain.retrievers import EnsembleRetriever
    from langchain_core.documents import Document
    from langchain.chains import RetrievalQA
    from langchain_core.prompts import PromptTemplate
    from langchain.retrievers.multi_query import MultiQueryRetriever
    from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
    from langchain.retrievers.document_compressors import LLMChainExtractor
    
    # Google AI Studio LLM import (ì•ˆì „í•œ ë°©ì‹)
    try:
        from langchain_google_genai import ChatGoogleGenerativeAI
        GOOGLE_LLM_AVAILABLE = True
    except Exception as e:
        logger.warning(f"Google LLM import ì‹¤íŒ¨: {e}")
        GOOGLE_LLM_AVAILABLE = False
        ChatGoogleGenerativeAI = None
        
except ImportError as e:
    logger.error(f"í•„ìˆ˜ íŒ¨í‚¤ì§€ import ì‹¤íŒ¨: {e}")
    raise

app = FastAPI(title="RAG ìµœì í™” ë¹„êµ ì‹œìŠ¤í…œ", version="2.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë°ì´í„° ëª¨ë¸ (Pydantic v2 í˜¸í™˜)
class QueryRequest(BaseModel):
    query: str = Field(..., description="ê²€ìƒ‰í•  ì§ˆë¬¸")
    optimization_methods: List[str] = Field(..., description="ì‚¬ìš©í•  RAG ìµœì í™” ë°©ë²•ë“¤")
    chunk_size: int = Field(default=1000, ge=100, le=4000, description="ì²­í¬ í¬ê¸°")
    chunk_overlap: int = Field(default=200, ge=0, le=1000, description="ì²­í¬ ì˜¤ë²„ë©")
    top_k: int = Field(default=5, ge=1, le=20, description="ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜")

class ComparisonResult(BaseModel):
    method: str
    response: str
    retrieved_docs: List[Dict[str, Any]]
    response_time: float
    relevance_score: float

class UploadResponse(BaseModel):
    message: str
    filename: str
    document_count: int

class SystemStatus(BaseModel):
    documents_loaded: bool
    document_count: int
    vectorstore_ready: bool
    llm_ready: bool
    api_key_configured: bool

# ì „ì—­ ë³€ìˆ˜
documents = []
vectorstore = None
google_api_key = os.getenv("GOOGLE_API_KEY")

if not google_api_key:
    logger.warning("âš ï¸  GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— ì¶”ê°€í•´ì£¼ì„¸ìš”.")

# LLM ì´ˆê¸°í™” (ì•ˆì „í•œ ë°©ì‹)
llm = None
if GOOGLE_LLM_AVAILABLE and google_api_key:
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            google_api_key=google_api_key,
            temperature=0.3
        )
        logger.info("âœ… Google LLM ì´ˆê¸°í™” ì„±ê³µ")
    except Exception as e:
        logger.error(f"Google LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        llm = None

# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (ì•ˆì „í•œ ë°©ì‹)
try:
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
except Exception as e:
    logger.error(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    embeddings = None

class RAGOptimizer:
    def __init__(self):
        self.methods = {
            "basic_rag": self.basic_rag,
            "multi_query": self.multi_query_rag,
            "ensemble_retrieval": self.ensemble_retrieval_rag,
            "contextual_compression": self.contextual_compression_rag,
            "hierarchical_chunking": self.hierarchical_chunking_rag,
            "semantic_chunking": self.semantic_chunking_rag
        }
        
    async def basic_rag(self, query: str, config: Dict) -> Dict:
        """ê¸°ë³¸ RAG êµ¬í˜„"""
        if not llm or not vectorstore:
            raise ValueError("LLM ë˜ëŠ” vectorstoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        start_time = time.time()
        
        try:
            retriever = vectorstore.as_retriever(search_kwargs={"k": config["top_k"]})
            
            prompt_template = """ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
            
            prompt = PromptTemplate(
                template=prompt_template,
                input_variables=["context", "question"]
            )
            
            qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=retriever,
                chain_type_kwargs={"prompt": prompt}
            )
            
            # ë¹„ë™ê¸° ì‹¤í–‰ì„ ë™ê¸°ë¡œ ë³€í™˜
            result = qa_chain.invoke({"query": query})["result"]
            docs = retriever.invoke(query)
            
            response_time = time.time() - start_time
            
            return {
                "method": "Basic RAG",
                "response": result,
                "retrieved_docs": [{"content": doc.page_content[:200] + "...", "metadata": doc.metadata} for doc in docs],
                "response_time": response_time,
                "relevance_score": self.calculate_relevance_score(query, docs)
            }
            
        except Exception as e:
            logger.error(f"Basic RAG ì˜¤ë¥˜: {e}")
            return {
                "method": "Basic RAG",
                "response": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "retrieved_docs": [],
                "response_time": time.time() - start_time,
                "relevance_score": 0.0
            }
    
    async def multi_query_rag(self, query: str, config: Dict) -> Dict:
        """Multi-Query RAG êµ¬í˜„"""
        if not llm or not vectorstore:
            raise ValueError("LLM ë˜ëŠ” vectorstoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        start_time = time.time()
        
        try:
            base_retriever = vectorstore.as_retriever(search_kwargs={"k": config["top_k"]})
            
            # Multi-query retriever ì„¤ì •
            multi_query_retriever = MultiQueryRetriever.from_llm(
                retriever=base_retriever,
                llm=llm
            )
            
            prompt_template = """ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
            
            prompt = PromptTemplate(
                template=prompt_template,
                input_variables=["context", "question"]
            )
            
            qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=multi_query_retriever,
                chain_type_kwargs={"prompt": prompt}
            )
            
            result = qa_chain.invoke({"query": query})["result"]
            docs = multi_query_retriever.invoke(query)
            
            response_time = time.time() - start_time
            
            return {
                "method": "Multi-Query RAG",
                "response": result,
                "retrieved_docs": [{"content": doc.page_content[:200] + "...", "metadata": doc.metadata} for doc in docs],
                "response_time": response_time,
                "relevance_score": self.calculate_relevance_score(query, docs)
            }
            
        except Exception as e:
            logger.error(f"Multi-Query RAG ì˜¤ë¥˜: {e}")
            return {
                "method": "Multi-Query RAG",
                "response": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "retrieved_docs": [],
                "response_time": time.time() - start_time,
                "relevance_score": 0.0
            }
    
    async def ensemble_retrieval_rag(self, query: str, config: Dict) -> Dict:
        """Ensemble Retrieval RAG êµ¬í˜„"""
        if not llm or not vectorstore:
            raise ValueError("LLM ë˜ëŠ” vectorstoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        start_time = time.time()
        
        try:
            # Vector retriever
            vector_retriever = vectorstore.as_retriever(search_kwargs={"k": config["top_k"]})
            
            # BM25 retriever
            bm25_retriever = BM25Retriever.from_documents(documents)
            bm25_retriever.k = config["top_k"]
            
            # Ensemble retriever
            ensemble_retriever = EnsembleRetriever(
                retrievers=[vector_retriever, bm25_retriever],
                weights=[0.6, 0.4]
            )
            
            prompt_template = """ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
            
            prompt = PromptTemplate(
                template=prompt_template,
                input_variables=["context", "question"]
            )
            
            qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=ensemble_retriever,
                chain_type_kwargs={"prompt": prompt}
            )
            
            result = qa_chain.invoke({"query": query})["result"]
            docs = ensemble_retriever.invoke(query)
            
            response_time = time.time() - start_time
            
            return {
                "method": "Ensemble Retrieval RAG",
                "response": result,
                "retrieved_docs": [{"content": doc.page_content[:200] + "...", "metadata": doc.metadata} for doc in docs],
                "response_time": response_time,
                "relevance_score": self.calculate_relevance_score(query, docs)
            }
            
        except Exception as e:
            logger.error(f"Ensemble Retrieval RAG ì˜¤ë¥˜: {e}")
            return {
                "method": "Ensemble Retrieval RAG",
                "response": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "retrieved_docs": [],
                "response_time": time.time() - start_time,
                "relevance_score": 0.0
            }
    
    async def contextual_compression_rag(self, query: str, config: Dict) -> Dict:
        """Contextual Compression RAG êµ¬í˜„"""
        # ê¸°ë³¸ RAGë¡œ í´ë°± (Google AI Studioì—ì„œ contextual compressionì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ)
        return await self.basic_rag(query, config)
    
    async def hierarchical_chunking_rag(self, query: str, config: Dict) -> Dict:
        """Hierarchical Chunking RAG êµ¬í˜„"""
        if not llm or not vectorstore:
            raise ValueError("LLM ë˜ëŠ” vectorstoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        start_time = time.time()
        
        try:
            # ê¸°ë³¸ ê²€ìƒ‰ ìˆ˜í–‰
            retriever = vectorstore.as_retriever(search_kwargs={"k": config["top_k"] * 2})
            
            prompt_template = """ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
            
            prompt = PromptTemplate(
                template=prompt_template,
                input_variables=["context", "question"]
            )
            
            qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=retriever,
                chain_type_kwargs={"prompt": prompt}
            )
            
            result = qa_chain.invoke({"query": query})["result"]
            docs = retriever.invoke(query)[:config["top_k"]]  # ìƒìœ„ Kê°œë§Œ ì„ íƒ
            
            response_time = time.time() - start_time
            
            return {
                "method": "Hierarchical Chunking RAG",
                "response": result,
                "retrieved_docs": [{"content": doc.page_content[:200] + "...", "metadata": doc.metadata} for doc in docs],
                "response_time": response_time,
                "relevance_score": self.calculate_relevance_score(query, docs)
            }
            
        except Exception as e:
            logger.error(f"Hierarchical Chunking RAG ì˜¤ë¥˜: {e}")
            return {
                "method": "Hierarchical Chunking RAG",
                "response": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "retrieved_docs": [],
                "response_time": time.time() - start_time,
                "relevance_score": 0.0
            }
    
    async def semantic_chunking_rag(self, query: str, config: Dict) -> Dict:
        """Semantic Chunking RAG êµ¬í˜„"""
        # ê¸°ë³¸ RAGì™€ ìœ ì‚¬í•˜ì§€ë§Œ ë‹¤ë¥¸ ì²­í‚¹ ì „ëµ ì‚¬ìš©
        return await self.basic_rag(query, config)
    
    def calculate_relevance_score(self, query: str, docs: List[Document]) -> float:
        """ê°„ë‹¨í•œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        if not docs:
            return 0.0
        
        try:
            # í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨í•œ ì ìˆ˜ ê³„ì‚°
            query_words = set(query.lower().split())
            scores = []
            
            for doc in docs:
                doc_words = set(doc.page_content.lower().split())
                intersection = len(query_words & doc_words)
                union = len(query_words | doc_words)
                if union > 0:
                    scores.append(intersection / union)
                else:
                    scores.append(0.0)
            
            return sum(scores) / len(scores) if scores else 0.0
        except Exception as e:
            logger.error(f"ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0

# RAG ìµœì í™”ê¸° ì¸ìŠ¤í„´ìŠ¤
rag_optimizer = RAGOptimizer()

@app.get("/")
async def root():
    return {"message": "RAG ìµœì í™” ë¹„êµ ì‹œìŠ¤í…œ API v2.0", "status": "running"}

@app.post("/upload", response_model=UploadResponse)
async def upload_document(file: UploadFile = File(...)):
    """ë¬¸ì„œ ì—…ë¡œë“œ ë° ì²˜ë¦¬"""
    global documents, vectorstore
    
    if not file.filename:
        raise HTTPException(status_code=400, detail="íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if not embeddings:
        raise HTTPException(status_code=500, detail="ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        # íŒŒì¼ ì €ì¥
        file_path = f"./uploads/{file.filename}"
        os.makedirs("./uploads", exist_ok=True)
        
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # ë¬¸ì„œ ë¡œë“œ
        if file.filename.lower().endswith('.pdf'):
            loader = PyPDFLoader(file_path)
        elif file.filename.lower().endswith('.txt'):
            loader = TextLoader(file_path, encoding='utf-8')
        elif file.filename.lower().endswith('.docx'):
            loader = Docx2txtLoader(file_path)
        else:
            raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. (PDF, TXT, DOCXë§Œ ì§€ì›)")
        
        # ë¬¸ì„œ ë¶„í• 
        docs = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            is_separator_regex=False,
        )
        documents = text_splitter.split_documents(docs)
        
        # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        vectorstore = Chroma.from_documents(
            documents, 
            embeddings,
            collection_name="rag_documents"
        )
        
        # íŒŒì¼ ì‚­ì œ
        os.remove(file_path)
        
        logger.info(f"ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ: {file.filename}, ì²­í¬ ìˆ˜: {len(documents)}")
        
        return UploadResponse(
            message="ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ê³  ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
            filename=file.filename,
            document_count=len(documents)
        )
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        # ì‹¤íŒ¨í•œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(file_path):
            os.remove(file_path)
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/compare")
async def compare_rag_methods(request: QueryRequest):
    """RAG ë°©ë²•ë“¤ì„ ë¹„êµí•˜ì—¬ ê²°ê³¼ ë°˜í™˜"""
    if not documents:
        raise HTTPException(status_code=400, detail="ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    
    if not llm:
        raise HTTPException(status_code=400, detail="Google AI Studio LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    config = {
        "chunk_size": request.chunk_size,
        "chunk_overlap": request.chunk_overlap,
        "top_k": request.top_k
    }
    
    results = []
    
    for method in request.optimization_methods:
        if method not in rag_optimizer.methods:
            logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°©ë²•: {method}")
            continue
            
        try:
            logger.info(f"{method} ì‹¤í–‰ ì¤‘...")
            result = await rag_optimizer.methods[method](request.query, config)
            results.append(result)
            logger.info(f"{method} ì™„ë£Œ - ì‘ë‹µ ì‹œê°„: {result['response_time']:.2f}s")
        except Exception as e:
            logger.error(f"{method} ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            results.append({
                "method": method,
                "response": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "retrieved_docs": [],
                "response_time": 0.0,
                "relevance_score": 0.0
            })
    
    return {"results": results, "query": request.query, "config": config}

@app.get("/methods")
async def get_available_methods():
    """ì‚¬ìš© ê°€ëŠ¥í•œ RAG ìµœì í™” ë°©ë²• ëª©ë¡ ë°˜í™˜"""
    methods = [
        {
            "id": "basic_rag",
            "name": "Basic RAG",
            "description": "ê¸°ë³¸ì ì¸ RAG êµ¬í˜„ (Vector Search + LLM)"
        },
        {
            "id": "multi_query",
            "name": "Multi-Query RAG", 
            "description": "ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í–¥ìƒ"
        },
        {
            "id": "ensemble_retrieval",
            "name": "Ensemble Retrieval",
            "description": "Vector Search + BM25 ê²°í•©"
        },
        {
            "id": "contextual_compression",
            "name": "Contextual Compression",
            "description": "ê´€ë ¨ ì •ë³´ë§Œ ì••ì¶•í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ìµœì í™”"
        },
        {
            "id": "hierarchical_chunking",
            "name": "Hierarchical Chunking",
            "description": "ê³„ì¸µì  ì²­í‚¹ìœ¼ë¡œ ì •ë³´ êµ¬ì¡°í™”"
        },
        {
            "id": "semantic_chunking", 
            "name": "Semantic Chunking",
            "description": "ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ì²­í‚¹"
        }
    ]
    
    return {"methods": methods}

@app.get("/status", response_model=SystemStatus)
async def get_status():
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
    return SystemStatus(
        documents_loaded=len(documents) > 0,
        document_count=len(documents),
        vectorstore_ready=vectorstore is not None,
        llm_ready=llm is not None,
        api_key_configured=google_api_key is not None
    )

if __name__ == "__main__":
    import uvicorn
    
    logger.info("ğŸš€ RAG ìµœì í™” ë¹„êµ ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    logger.info(f"ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„: {'âœ…' if embeddings else 'âŒ'}")
    logger.info(f"Google LLM ì¤€ë¹„: {'âœ…' if llm else 'âŒ'}")
    logger.info(f"API í‚¤ ì„¤ì •: {'âœ…' if google_api_key else 'âŒ'}")
    
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=False)